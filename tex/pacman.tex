%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[a4paper, 11pt]{article} % Font size (can be 10pt, 11pt or 12pt) and paper size (remove a4paper for US letter paper)

\usepackage{standalone}
\usepackage{amsmath}

\usepackage[a4paper,left=3cm,right=3cm]{geometry}

\usepackage{indentfirst}
%\usepackage[utf8]{inputenc}
\usepackage{pgfplots}
\usepackage{color}

\usepackage{enumerate}

\usepackage[protrusion=true,expansion=true]{microtype} % Better typography
\usepackage{graphicx} % Required for including pictures
\usepackage{wrapfig} % Allows in-line images

\usepackage{tikz}
\usetikzlibrary{shapes}
\usepackage{pgf}
\usepackage{pgfplots}
\usetikzlibrary{arrows}
\usepackage{tkz-graph}
\usetikzlibrary{shapes.multipart}

\renewcommand{\thesection}{\Roman{section}}
\usepackage{titlesec}
%\titlespacing\section{0pt}{20pt plus 4pt minus 2pt}{-5pt plus 2pt minus 2pt}

\usepackage{listings}
\usepackage{textcomp}
\usepackage{xcolor}
\lstset{
%language=Java,
basicstyle=\normalsize, % ou \c ca==> basicstyle=\scriptsize,
upquote=true,
aboveskip={1.2\baselineskip},
columns=fullflexible,
showstringspaces=false,
extendedchars=true,
breaklines=true,
showtabs=false,
showspaces=false,
showstringspaces=false,
identifierstyle=\ttfamily,
keywordstyle=\color[rgb]{0,0,1},
commentstyle=\color[rgb]{0.133,0.545,0.133},
stringstyle=\color[rgb]{0.627,0.126,0.941},
}

\lstset{% This applies to ALL lstlisting
    backgroundcolor=\color{yellow!10},%
    numbers=left, numberstyle=\tiny, stepnumber=2, numbersep=5pt,%
    }%

% Applies only when you use it
\lstdefinestyle{MyLang}{
    basicstyle=\small\ttfamily\color{magenta},%
    breaklines=true,%                                      allow line breaks
    moredelim=[s][\color{green!50!black}\ttfamily]{'}{'},% single quotes in green
    moredelim=*[s][\color{black}\ttfamily]{options}{\}},%  options in black (until trailing })
    commentstyle={\color{gray}\itshape},%                  gray italics for comments
    morecomment=[l]{//},%                                  define // comment
    emph={%
        STRING%                                            literal strings listed here
        },emphstyle={\color{blue}\ttfamily},%              and formatted in blue
    alsoletter={:,|,;},%
    morekeywords={:,|,;},%                                 define the special characters
    keywordstyle={\color{black}},%                         and format them in black
}

\lstdefinestyle{Pyth}{
	language=Python
}


\usepackage{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Required for accented characters
\linespread{1.05} % Change line spacing here, Palatino benefits from a slight increase by default

\makeatletter
\renewcommand\@biblabel[1]{\textbf{#1.}} % Change the square brackets for each bibliography item from '[1]' to '1.'
\renewcommand{\@listI}{\itemsep=0pt} % Reduce the space between items in the itemize and enumerate environments and the bibliography

\renewcommand{\maketitle}{ % Customize the title - do not edit title and author name here, see the TITLE block below
\begin{flushright} % Right align
{\LARGE\@title} % Increase the font size of the title

\vspace{50pt} % Some vertical space between the title and author name

{\large\@author} % Author name
\\\@date % Date

\vspace{-20pt} % Some vertical space between the author block and abstract
\end{flushright}
}

%----------------------------------------------------------------------------------------
%	TITLE
%----------------------------------------------------------------------------------------

\title{\textbf{Intelligence Artificielle}\\ % Title
Deep Q-Learning for PAC-MAN} % Subtitle

\author{\textsc{Louis Annabi - Fran\c cois Hernandez - L\'eo Pons} % Author
\\{\textit{CentraleSup\'elec}}} % Institution

\date{\today} % Date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title section

\pagebreak
\tableofcontents

\pagebreak

%----------------------------------------------------------------------------------------
%	ABSTRACT AND KEYWORDS
%----------------------------------------------------------------------------------------

%\renewcommand{\abstractname}{Summary} % Uncomment to change the name of the abstract to something else



\vspace{30pt} % Some vertical space between the abstract and first section

%----------------------------------------------------------------------------------------
%	ESSAY BODY
%----------------------------------------------------------------------------------------

\section*{Introduction}
\addcontentsline{toc}{section}{Introduction}

Blabla


%------------------------------------------------
\pagebreak

\section{Description du problème}

\subsection{Processus de décision Markovien}

Pacman est un jeu non déterministe puisque l'état suivant n'est pas uniquement déterminée par l'action du joueur. En effet, lorsque les fantômes se trouvent à des croisements, ils peuvent choisir leur direction de manière aléatoire. \\

Pour représenter ce non-déterminisme, on modélise notre jeu comme un processus de décision markovien (MDP). Un processus de décision markovien est un processus stochastique basé sur l'hypothèse de Markov : les probabilités de transitions ne dépendent que des $n$ états précédents. \\

Plus formellement, un processus de décision markovien est défini par :
\begin{itemize}
\item Un ensemble d'états $s \in  S$
\item Un ensemble d'actions $a \in  A$
\item Une fonction de transition $T(s,a,s')$
\item Une fonction de récompense $R(s,a,s')$
\item Un état de départ $start \in S$
\end{itemize}

Avec ce formalisme, le but du jeu est de déterminer une politique de déplacement $\pi : S \rightarrow A$ qui maximise le total des récompenses $R(s,a,s')$ obtenues sur toute la durée de la partie.

\subsection{Pacman comme MDP}

Le fonctionnement du Pacman correspond bien à un processus de décision markovien. En effet les probabilités de transitions d'un état $s$ à un état $s'$ ne dépendent que de l'état $s$ et de l'état précédant $s$. L'ensemble des actions possibles est alors $A = \{up, down, left, right, stay\}$ et un état $s$ est défini par la position du Pacman, ainsi que par les positions des récompenses et des fantômes.\\

Le non-déterminisme n'opère qu'au niveau des déplacements des fantômes, avec comme seule contrainte qu'il ne peuvent pas faire de demi-tour. Pour connaître la probabilité de transition vers un état $s'$ il faut connaitre l'état $s$ ainsi que les directions courantes des fantômes, qui sont données par la connaissance de l'état précédant $s$. \\

\begin{figure}[h]
	\includegraphics[scale=0.5]{img/non-determinism.png}
	\caption{Non-déterminisme dans Pacman.}
\end{figure}

L'image ci-dessous représente une succession d'états. Nous voyons que pour passer de l'état 1 à l'état 2, seule l'action du joueur importe. En revanche, si le joueur continue d'aller à gauche à l'état 2, l'état suivant peut être soit A soit B. \\

En reprenant le formalisme introduit précédemment, on a :

\begin{eqnarray}
T(s_1, left, s_2) = 1
\end{eqnarray} 
\begin{eqnarray}
T(s_2, left, s_A) = 0.5
\end{eqnarray} 
\begin{eqnarray}
T(s_2, left, s_B) = 0.5
\end{eqnarray} 

%------------------------------------------------
\pagebreak

\section{Solutions envisageables (Léo)}

\subsection{Définitions}

Le but du problème est donc de déterminer une politique $\pi : S \rightarrow A$ permettant de choisir à chaque instant l'action à effectuer maximisant la la récompense totale espérée $R$ décrite en partie précédente.\\

On considèrera pour cela la value function qui renseigne l'espérance du résultat pour une politique et un état initial donnés :

\begin{eqnarray}
V_\pi(s_0) = E[R\mid(s_0,\pi)]
\end{eqnarray}
\vspace{0.1cm}

Trouver une politique optimale revient donc à maximiser $V_\pi$. On notera sa forme optimale :

\begin{eqnarray}
V_{opt}(s_0) = \arg\max_{\pi} V_\pi(s_0)
\end{eqnarray}
\vspace{0.1cm}

On définit d'une manière similaire la Q-value qui renseigne l'espérance du résultat pour une politique et un état initial donnés, à la différence cette fois-ci que la première action effectuée $a_0$ est déterminée. La politique n'est appliquée que pour les actions suivantes. 

\begin{eqnarray}
Q_\pi(s_0,a_0) = E[R\mid(s_0,a_0,\pi)]
\end{eqnarray}
\vspace{0.1cm}

Et on notera de la même manière $Q_{opt}$ la Q-value pour une politique optimale.\\

Pour agir de manière optimale, il suffit donc de connaître $Q_{opt}$. En effet à tout instant, dans un état donné, il suffit de choisir l'action maximisant $Q_{opt}$. En raisonnant récursivement, on sait que la suite de nos actions seront optimales, ce qui valide le choix de notre action (qui n'est acceptable que si suivi d'une politique optimale, d'après la définition de la Q-value (3) ). On note donc :

\begin{eqnarray}
\pi_{opt}(s) = \arg\max_{a \in A} Q_{opt}(s,a)
\end{eqnarray}
\vspace{0.1cm}


On peut aussi retrouver $Q$ à partir de $V$ :

\begin{eqnarray}
Q_{\pi}(s,a) = \sum\limits_{s'} T(s,a,s')(Reward(s,a,s') + \gamma V_\pi(s')
\end{eqnarray}
\vspace{0.1cm}

Où, comme précisé en première partie, $T(s,a,s')$ est la probabilité de tomber dans l'état $s'$ suite au choix de l'action $a$ depuis $s$, $Reward(s,a,s')$ la récompense associée à cette transition, et $\gamma$ le facteur de dévaluation temporel.\\

Ces définitions étant données, plusieurs premières approches sont envisageables.

\subsection{Value iteration}

En supposant la connaissance complète du système, un première approche consiste à calculer la politique ou la value function itérativement, en précisant leurs valeurs jusqu'à convergence. Plusieurs variantes existent, la plus notable étant la "value iteration", où l'on procède uniquement par itération sur la value function :

\begin{eqnarray}
V_{i+1}(s) := \max_{a \in A} \Bigl\{ \sum\limits_{s'} T(s,a,s')(Reward(s,a,s') + \gamma V_i(s') \Bigr\}
\end{eqnarray}
\vspace{0.1cm}

Pour tout $s \in S$ on initialise $V_0(s)$ aléatoirement ou par intuition, puis à chaque itération, on met à jour la valeur de $V$ selon la formule ci-dessus, toujours pour chaque état $s$ possible. On continue jusqu'à convergence, ce qui nous donne ainsi les valeurs $V_{opt}(s)$ pour tout $s$. On peut ainsi facilement retrouver $Q_{opt}$ à l'aide de (5) et donc notre politique optimale avec (4).\\

Le problème de ce type de stratégie est la nécessité de calculer à l'avance $V$ pour tous les états $s$ envisageables, ce qui est vite problématique quand l'espace des états est grand. En particulier, pour le Pacman, les états résultants des différentes combinaisons des positions du Pacman, des fantômes, de la nourriture etc. le nombre d'états est beaucoup trop grand pour procéder ainsi.


\subsection{Reinforcement Learning}

\subsection{Deep Q-Learning}


%------------------------------------------------
\pagebreak

\section{Modèles et résultats}

FRANCOIS

%------------------------------------------------

%\pagebreak
%\section*{Conclusion}
%\addcontentsline{toc}{section}{Conclusion}


%----------------------------------------------------------------------------------------

\end{document}