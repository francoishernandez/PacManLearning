%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[a4paper, 11pt]{article} % Font size (can be 10pt, 11pt or 12pt) and paper size (remove a4paper for US letter paper)

\usepackage{standalone}
\usepackage{amsmath}

\usepackage[a4paper,left=3cm,right=3cm]{geometry}

\usepackage{indentfirst}
%\usepackage[utf8]{inputenc}
\usepackage{pgfplots}
\usepackage{color}

\usepackage{enumerate}

\usepackage[protrusion=true,expansion=true]{microtype} % Better typography
\usepackage{graphicx} % Required for including pictures
\usepackage{wrapfig} % Allows in-line images

\usepackage{tikz}
\usetikzlibrary{shapes}
\usepackage{pgf}
\usepackage{pgfplots}
\usetikzlibrary{arrows}
\usepackage{tkz-graph}
\usetikzlibrary{shapes.multipart}

\renewcommand{\thesection}{\Roman{section}}
\usepackage{titlesec}
%\titlespacing\section{0pt}{20pt plus 4pt minus 2pt}{-5pt plus 2pt minus 2pt}

\usepackage{listings}
\usepackage{textcomp}
\usepackage{xcolor}
\lstset{
%language=Java,
basicstyle=\normalsize, % ou \c ca==> basicstyle=\scriptsize,
upquote=true,
aboveskip={1.2\baselineskip},
columns=fullflexible,
showstringspaces=false,
extendedchars=true,
breaklines=true,
showtabs=false,
showspaces=false,
showstringspaces=false,
identifierstyle=\ttfamily,
keywordstyle=\color[rgb]{0,0,1},
commentstyle=\color[rgb]{0.133,0.545,0.133},
stringstyle=\color[rgb]{0.627,0.126,0.941},
}

\lstset{% This applies to ALL lstlisting
    backgroundcolor=\color{yellow!10},%
    numbers=left, numberstyle=\tiny, stepnumber=2, numbersep=5pt,%
    }%

% Applies only when you use it
\lstdefinestyle{MyLang}{
    basicstyle=\small\ttfamily\color{magenta},%
    breaklines=true,%                                      allow line breaks
    moredelim=[s][\color{green!50!black}\ttfamily]{'}{'},% single quotes in green
    moredelim=*[s][\color{black}\ttfamily]{options}{\}},%  options in black (until trailing })
    commentstyle={\color{gray}\itshape},%                  gray italics for comments
    morecomment=[l]{//},%                                  define // comment
    emph={%
        STRING%                                            literal strings listed here
        },emphstyle={\color{blue}\ttfamily},%              and formatted in blue
    alsoletter={:,|,;},%
    morekeywords={:,|,;},%                                 define the special characters
    keywordstyle={\color{black}},%                         and format them in black
}

\lstdefinestyle{Pyth}{
	language=Python
}


\usepackage{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Required for accented characters
\linespread{1.05} % Change line spacing here, Palatino benefits from a slight increase by default

\makeatletter
\renewcommand\@biblabel[1]{\textbf{#1.}} % Change the square brackets for each bibliography item from '[1]' to '1.'
\renewcommand{\@listI}{\itemsep=0pt} % Reduce the space between items in the itemize and enumerate environments and the bibliography

\renewcommand{\maketitle}{ % Customize the title - do not edit title and author name here, see the TITLE block below
\begin{flushright} % Right align
{\LARGE\@title} % Increase the font size of the title

\vspace{50pt} % Some vertical space between the title and author name

{\large\@author} % Author name
\\\@date % Date

\vspace{-20pt} % Some vertical space between the author block and abstract
\end{flushright}
}

%----------------------------------------------------------------------------------------
%	TITLE
%----------------------------------------------------------------------------------------

\title{\textbf{Intelligence Artificielle}\\ % Title
Deep Q-Learning for PAC-MAN} % Subtitle

\author{\textsc{Louis Annabi - Fran\c cois Hernandez - L\'eo Pons} % Author
\\{\textit{CentraleSup\'elec}}} % Institution

\date{\today} % Date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title section

\pagebreak
\tableofcontents

\pagebreak

%----------------------------------------------------------------------------------------
%	ABSTRACT AND KEYWORDS
%----------------------------------------------------------------------------------------

%\renewcommand{\abstractname}{Summary} % Uncomment to change the name of the abstract to something else



\vspace{30pt} % Some vertical space between the abstract and first section

%----------------------------------------------------------------------------------------
%	ESSAY BODY
%----------------------------------------------------------------------------------------

\section*{Introduction}
\addcontentsline{toc}{section}{Introduction}

Blabla


%------------------------------------------------
\pagebreak

\section{Description du problème}

LOUIS

%------------------------------------------------
\pagebreak

\section{Solutions envisageables}

Le but du problème est donc de déterminer une politique $\pi : S \rightarrow A$ permettant de choisir à chaque instant $t$ l'action à effectuer maximisant la la récompense totale espérée $R$ décrite en partie précédente.\\

On considèrera pour cela la value function qui renseigne l'espérance du résultat pour une politique et un état initial donnés :

\begin{eqnarray}
V_\pi(s_0) = E[R\mid(s_0,\pi)]
\end{eqnarray}
\vspace{0.1cm}

Trouver une politique optimale revient donc à maximiser $V_\pi$. On notera sa forme optimale :

\begin{eqnarray}
V_{opt}(s_0) = \arg\max_{\pi} V_\pi(s_0)
\end{eqnarray}
\vspace{0.1cm}

On définit d'une manière similaire la Q-value qui renseigne l'espérance du résultat pour une politique et un état initial donnés, à la différence cette fois-ci que la première action effectuée $a_0$ est déterminée. La politique n'est appliquée que pour les actions suivantes. 

\begin{eqnarray}
Q_\pi(s_0,a_0) = E[R\mid(s_0,a_0,\pi)]
\end{eqnarray}
\vspace{0.1cm}

Et on notera de la même manière $Q_{opt}$ la Q-value pour une politique optimale.\\

Pour agir de manière optimale, il suffit donc de connaître $Q_{opt}$. En effet à tout instant, dans un état donné, il suffit de choisir l'action maximisant $Q_{opt}$. En raisonnant récursivement, on sait que la suite de nos actions seront optimales, ce qui valide le choix de notre action (qui n'est acceptable que si suivi d'une politique optimale, d'après la définition de la Q-value). On note donc :

\begin{eqnarray}
\pi_{opt}(s) = \arg\max_{a \in A} Q_{opt}(s,a)
\end{eqnarray}
\vspace{0.1cm}

Ces définitions étant données, plusieurs premières approches sont envisageables :\\

\subsection{Value iteration}

\subsection{Reinforcement Learning}

\subsection{Deep Q-Learning}


%------------------------------------------------
\pagebreak

\section{Modèles et résultats}

Nous allons dans cette partie nous intéresser aux modèles présentés et implémentés dans l'article \textit{Recurrent Deep Q-Learning for PAC-MAN} (Kushal Ranjan, Amelia Christensen, Bernardo Ramos, \textsc{Stanford}) ainsi qu'aux résultats obtenus.

But : developper un agent d'apprentissage renforcé pour PAC-MAN, avec une combinaison de CNN et RNN, et des unités LSTM pour modéliser la Q-value function Q(s,a).

\subsection{Reinforcement learning}

La théorie a été présentée en détail dans la partie XXX.\\

En résumé, nous disposons d'une chaîne de décision de Markov comportant l'ensemble des configurations de PAC-MAN (position, fantômes, nourriture) ainsi que l'ensemble des actions possibles (gauche, droite, haut, bas, rester). L'idée est de maximiser la value function $V_{\pi}$ pour trouver la politique optimale $\pi$. La Q-value a donc été définie de la façon suivante :
\begin{eqnarray}
Q_{\pi}(s,a) = \sum_{s'} T(s,a,s')(Reward(s,a,s')+\gamma V_{\pi}(s'))
\end{eqnarray}

La condition est donc :

\begin{eqnarray}
\pi_{opt}(s) = \operatorname{arg\,max}_{a\in A} Q_{\pi_{opt}}(s,a) \forall s \in S
\end{eqnarray}

L'idée est d'estimer la Q-function d'une politique optimale $Q_{opt}$ et d'utiliser la formule XXX pour trouver $\pi_{opt}$.

Pour cela, un moyen est d'entraîner un réseau de neurones de paramètres $\theta$ avec la fonction de perte suivante :

\begin{eqnarray}
L(\theta) = \sum_{(s,a,r,s')} (Q_{opt}(s,a;\theta) - (r+\gamma V_{opt}))
\end{eqnarray}

\subsection{Architectures}

\subsubsection{Inception-LSTM}

La première architecture vise à tester l'effet du transfer learning sur la capacité d'apprentissage du réseau. Le transfer learning, discipline à part entière, consiste à utiliser une partie d'un réseau déjà entraîné pour extraire des features dans une autre architecture. L'architecture suit le déroulement suivant :
\begin{enumerate}
\item extraction de features à partir des images brutes, utilisation de Google Inception v3 (fourth max-pool) : EXPLIQUER ? ;
\item alimentation d'un fully connected layer avec les features extraites ;
\item ajout d'une couche LSTM (Long Short-Term Memory, les états cachés contiennent la mémoire des actions passées) ;
\item récupération des Q-values pour les cinq actions via une couche FC-5 (fully connected, couche de sortie de taille 5).
\end{enumerate}

L'architecture est résumée par le schéma figure XXX.

\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{inception-lstm.png}
\caption{Architecture Inception-LSTM}
\end{center}
\end{figure}

\subsubsection{Convolutional-LSTM}

Ensuite, une architecture entraînée à partir de zéro est proposée. Elle suit le déroulement suivant :
\begin{enumerate}
\item trois couches de pooling (avec couche de correction) ;
\item deux couches de réseaux fully connected FC1 et FC2 ;
\item ajout d'une couche LSTM ;
\item récupération des Q-values pour les cinq actions via une couche FC-5 (fully connected, couche de sortie de taille 5).
\end{enumerate}

L'architecture est résumée par le schéma figure XXX.

\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{conv-lstm.png}
\caption{Architecture Inception-LSTM}
\end{center}
\end{figure}

\subsubsection{CNN et}

\subsection{Supervised Learning}

\subsection{Resultats}

\subsection{Extraction des features}

\paragraph{Avantages de l'ajout d'un Fully Connected layer plutôt qu'un Fully Convolutional}

\begin{itemize}
\item input image size ;
\item spatial information ;
\item computational cost and representation power ;
\end{itemize}


Ajout de LSTM --> ajout de robustesse

FRANCOIS

%------------------------------------------------

%\pagebreak
%\section*{Conclusion}
%\addcontentsline{toc}{section}{Conclusion}


%----------------------------------------------------------------------------------------

\end{document}