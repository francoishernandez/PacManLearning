%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[a4paper, 11pt]{article} % Font size (can be 10pt, 11pt or 12pt) and paper size (remove a4paper for US letter paper)

\usepackage{standalone}
\usepackage{amsmath}

\usepackage[a4paper,left=3cm,right=3cm]{geometry}

\usepackage{indentfirst}
%\usepackage[utf8]{inputenc}
\usepackage{pgfplots}
\usepackage{color}

\usepackage{enumerate}

\usepackage[protrusion=true,expansion=true]{microtype} % Better typography
\usepackage{graphicx} % Required for including pictures
\usepackage{wrapfig} % Allows in-line images

\usepackage{tikz}
\usetikzlibrary{shapes}
\usepackage{pgf}
\usepackage{pgfplots}
\usetikzlibrary{arrows}
\usepackage{tkz-graph}
\usetikzlibrary{shapes.multipart}

\renewcommand{\thesection}{\Roman{section}}
\usepackage{titlesec}
%\titlespacing\section{0pt}{20pt plus 4pt minus 2pt}{-5pt plus 2pt minus 2pt}

\usepackage{listings}
\usepackage{textcomp}
\usepackage{xcolor}
\lstset{
%language=Java,
basicstyle=\normalsize, % ou \c ca==> basicstyle=\scriptsize,
upquote=true,
aboveskip={1.2\baselineskip},
columns=fullflexible,
showstringspaces=false,
extendedchars=true,
breaklines=true,
showtabs=false,
showspaces=false,
showstringspaces=false,
identifierstyle=\ttfamily,
keywordstyle=\color[rgb]{0,0,1},
commentstyle=\color[rgb]{0.133,0.545,0.133},
stringstyle=\color[rgb]{0.627,0.126,0.941},
}

\lstset{% This applies to ALL lstlisting
    backgroundcolor=\color{yellow!10},%
    numbers=left, numberstyle=\tiny, stepnumber=2, numbersep=5pt,%
    }%

% Applies only when you use it
\lstdefinestyle{MyLang}{
    basicstyle=\small\ttfamily\color{magenta},%
    breaklines=true,%                                      allow line breaks
    moredelim=[s][\color{green!50!black}\ttfamily]{'}{'},% single quotes in green
    moredelim=*[s][\color{black}\ttfamily]{options}{\}},%  options in black (until trailing })
    commentstyle={\color{gray}\itshape},%                  gray italics for comments
    morecomment=[l]{//},%                                  define // comment
    emph={%
        STRING%                                            literal strings listed here
        },emphstyle={\color{blue}\ttfamily},%              and formatted in blue
    alsoletter={:,|,;},%
    morekeywords={:,|,;},%                                 define the special characters
    keywordstyle={\color{black}},%                         and format them in black
}

\lstdefinestyle{Pyth}{
	language=Python
}


\usepackage{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Required for accented characters
\linespread{1.05} % Change line spacing here, Palatino benefits from a slight increase by default

\makeatletter
\renewcommand\@biblabel[1]{\textbf{#1.}} % Change the square brackets for each bibliography item from '[1]' to '1.'
\renewcommand{\@listI}{\itemsep=0pt} % Reduce the space between items in the itemize and enumerate environments and the bibliography

\renewcommand{\maketitle}{ % Customize the title - do not edit title and author name here, see the TITLE block below
\begin{flushright} % Right align
{\LARGE\@title} % Increase the font size of the title

\vspace{50pt} % Some vertical space between the title and author name

{\large\@author} % Author name
\\\@date % Date

\vspace{-20pt} % Some vertical space between the author block and abstract
\end{flushright}
}

%----------------------------------------------------------------------------------------
%	TITLE
%----------------------------------------------------------------------------------------

\title{\textbf{Intelligence Artificielle}\\ % Title
Deep Q-Learning for PAC-MAN} % Subtitle

\author{\textsc{Louis Annabi - Fran\c cois Hernandez - L\'eo Pons} % Author
\\{\textit{CentraleSup\'elec}}} % Institution

\date{\today} % Date

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Print the title section

\pagebreak
\tableofcontents

\pagebreak

%----------------------------------------------------------------------------------------
%	ABSTRACT AND KEYWORDS
%----------------------------------------------------------------------------------------

%\renewcommand{\abstractname}{Summary} % Uncomment to change the name of the abstract to something else



\vspace{30pt} % Some vertical space between the abstract and first section

%----------------------------------------------------------------------------------------
%	ESSAY BODY
%----------------------------------------------------------------------------------------

\section*{Introduction}
\addcontentsline{toc}{section}{Introduction}

Blabla


%------------------------------------------------
\pagebreak

\section{Description du problème}

LOUIS

%------------------------------------------------
%\pagebreak

\section{Solutions envisageables}

LEO

%------------------------------------------------
\pagebreak

\section{Modèles et résultats}

Nous allons dans cette partie nous intéresser aux modèles présentés et implémentés dans l'article \textit{Recurrent Deep Q-Learning for PAC-MAN} (Kushal Ranjan, Amelia Christensen, Bernardo Ramos, \textsc{Stanford}) ainsi qu'aux résultats obtenus.

But : developper un agent d'apprentissage renforcé pour PAC-MAN, avec une combinaison de CNN et RNN, et des unités LSTM pour modéliser la Q-value function Q(s,a).

\subsection{Reinforcement learning}

La théorie a été présentée en détail dans la partie XXX.\\

En résumé, nous disposons d'une chaîne de décision de Markov comportant l'ensemble des configurations de PAC-MAN (position, fantômes, nourriture) ainsi que l'ensemble des actions possibles (gauche, droite, haut, bas, rester). L'idée est de maximiser la value function $V_{\pi}$ pour trouver la politique optimale $\pi$. La Q-value a donc été définie de la façon suivante :
\begin{eqnarray}
Q_{\pi}(s,a) = \sum_{s'} T(s,a,s')(Reward(s,a,s')+\gamma V_{\pi}(s'))
\end{eqnarray}

La condition est donc :

\begin{eqnarray}
\pi_{opt}(s) = \operatorname{arg\,max}_{a\in A} Q_{\pi_{opt}}(s,a) \forall s \in S
\end{eqnarray}

L'idée est d'estimer la Q-function d'une politique optimale $Q_{opt}$ et d'utiliser la formule XXX pour trouver $\pi_{opt}$.

Pour cela, un moyen est d'entraîner un réseau de neurones de paramètres $\theta$ avec la fonction de perte suivante :
$L(\theta) = \sum_{(s,a,r,s')} (Q_{opt}(s,a;\theta) - (r+\gamma V_{opt}))

\subsection{Deep Q-Learning and Convolutional Neural Networks}

\subsection{Recurrent Neural Networks}

\subsection{Architectures}



\subsubsection{LSTM et CNN}

\subsubsection{CNN et RNN}

\subsubsection{CNN et 

\subsection{Supervised Learning}

\subsection{Resultats}


Ajout de LSTM --> ajout de robustesse

FRANCOIS

%------------------------------------------------

%\pagebreak
%\section*{Conclusion}
%\addcontentsline{toc}{section}{Conclusion}


%----------------------------------------------------------------------------------------

\end{document}